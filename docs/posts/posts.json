[
  {
    "path": "posts/2021-02-20-california-oil-spills/",
    "title": "California Oil Spills",
    "description": "Choropleth Map Using `ggplot2`",
    "author": [
      {
        "name": "Kathleen Cotti",
        "url": {}
      }
    ],
    "date": "2021-02-20",
    "categories": [],
    "contents": "\nSummary:\nThis post explores how to build a static choropleth map with R and the ggplot2 package. It shows how to load spatial data into R, merge region features between datasets, and build a finalized map of 2008 counts of inland oil spills in California counties as a fill color gradient.\nI first had issues plotting the oil spills as fill color on the map when I used two separate geom_sf() layers, but I realized I could merge the data frame and then use the number column aesthetically as the fill color of the map. In doing this project, I became more familiar with reading in and wrangling spatial data, using the sf package functions, and how to use the viridis package for color gradients. I also used the guide function to update my legend.\nFinding & Downloading Spatial Data:\nA choropleth map requires a geospatial dataset to provide the region boundaries, in this example we are looking at California counties. First, I had to find & download a shapefile data for borders of California counties.\nYou also need another geospatial dataset with numeric variables for each location that will we use to color the counties. I used download data from CA DFW Oil Spill Incident Tracking URL\nFirst, I loaded the spatial data sets into R using the read_sf() funtion from the sf package. I created a folder in my project root named “data” to store all data files & then the oil spatial data files are in a folder labeled “Oil_Spill_Tracking” and the California counties boundary data in a folder labeled “ca_counties”. I then use the here() function from the janitor package to give a clear file path to the shape files.\n\n\nhide\n\n# Read in the oil spatial data: \noil_spill_data <- read_sf(here(\"data\",\"Oil_Spill_Tracking\",\"Oil_Spill_Incident_Tracking_%5Bds394%5D.shp\")) %>% \n  clean_names()\n\n# Read in the CA counties data as an entire layer:\nca_counties <- read_sf(here(\"data\",\"ca_counties\"), layer = \"CA_Counties_TIGER2016\") %>% \n  clean_names() %>% \n  select(name) \n\n\n\nNext, I checked the projections, or how the coordinate reference systems are displayed, of the spatial data sets using the st_crs() function from the sf package. When merging two data sets to make this map, it is important that the projections of both data sets match or you’ll encounter difficulties in analysis & mapping. If the datasets have different projections, the st_transform() function can be used to transform one spatial data set to match the other, as shown below:\n\n\nhide\n\n#Check the Projections:\n# st_crs(oil_spill_data) \n# WGS 84\n\n# st_crs(ca_counties)\n# WGS 84\n\n#Set this data set to have the same CRS as above:\nca_counties <- st_transform(ca_counties, st_crs(oil_spill_data))\n\n\n\nData Wrangling of Oil Data:\nFor this analysis, I specifically plotted the number of inland oil spills per California county in 2008. Therefore, some data wrangling using functions from the dplyr package was necessary to get counts per county:\n\n\nhide\n\n# Data Wrangling - count oil spills for ca county:\n## Make a subset of data:\ninland_subset <- oil_spill_data %>% \n# Rename county for simplificaiton:\n  rename(county = localecoun) %>% \n# Only keep obervations from inland areas:\n  filter(inlandmari == \"Inland\") %>% \n# Group oil spills by county:\n  group_by(county) %>% \n# Count the number of inland oil spills per county:\n  count(inlandmari) %>% \n# Remove the inland column for simplification:\n  select(-inlandmari)\n\n\n\nMerge the spatial data sets:\nIn order to plot the oil data with the California counties outlines, I merged the data so I could plot the number of oil spills as a fill color gradient in the map\n\n\nhide\n\n# Join the data sets into a single data frame:\ncounties_oil <- st_join(ca_counties, inland_subset) \n\n\n\nMake a customized chloropleth map with ggplot2:\n\n\nhide\n\n#Make a Static Chloroleth Map:\nggplot() +\n  geom_sf(data = counties_oil,\n          aes(fill = n),\n          size = 0,\n          color = \"white\") +\n  scale_fill_viridis(option = \"inferno\", \n                     breaks = c(50, 100, 150, 200, 250, 300, 350, 400),\n                     begin = 0.1, \n                     end = 0.9,\n                     guide = guide_legend(keyheight = unit(5, units = \"mm\"), keywidth=unit(5, units = \"mm\"))) +\n  theme_void() +\n  labs(title = \"2008 Inland Oil Spill Events Across All California Counties\",\n       fill = \"Number of Oil Spills\") +\n  theme(\n    plot.background = element_rect(fill = \"#f5f5f3\", color = NA),\n    plot.title = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-20-california-oil-spills/california-oil-spills_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-02-20T16:27:05-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-20-harry-potter-text-analysis/",
    "title": "Harry Potter & The Sorcerer's Stone",
    "description": "Text Data Wrangling, Word Cloud Using `ggplot` & Sentiment Analysis",
    "author": [
      {
        "name": "Kathleen Cotti",
        "url": {}
      }
    ],
    "date": "2021-02-20",
    "categories": [],
    "contents": "\nSummary:\nThis post explores how to build a word cloud using ggplot & conduct sentiment analysis using the NRC lexicon. It shows how to load in pdf text data in R, get text data in tidy & tokenized format, and make finalized data visualizations for word frequencies & sentiment in Harry Potter & The Sorcerer’s Stone.\nThis was my first attempt at text analysis, and I had so much fun wrangling one of my favorite books into cool data visualizations. I became more familiar with using the stringr functions, and how to read in and tidy text. I ran into issues with my sentiment analysis plot since the chapter numbers were written out in words instead of as numbers, but I used mutate(case_when = ) to change them to numbers. I also practiced using scales = FREE to tidy up by sentiment data visualization. I also strengthened my skills in ggplot using then theme function to update the axis text size, and played around with background color in my first data visualization.\nReading in PDF Text Files:\n\n\nhide\n\nharry_potter_text <- pdf_text(here(\"data\", \"harry_potter.pdf\"))\n\n\n\nTidying up the Text Data:\nFirst, the text data must be transformed into a data frame using the data.frame function. This will make each page of the book into a separate row in the new data set. Then I did the following steps to tidy the text data frame: - Removed the pages before Chapter 1 starts using the slice() function - Removed excess white space using str_squish - Made a new column for chapter & populated when the row has “CHAPTER” in it & populated with NA for the other rows. Then used fill() to populate with the chapter number & used separate to separate into the word chapter and the number.\n\n\nhide\n\n# Make a tidy data subset:\nharry_potter_tidy <- data.frame(harry_potter_text) %>% \n# Remove the lines in the data frame before Chapter 1: \n  slice(-(1:6)) %>% \n# Remove excess white spaces: \n  mutate(harry_potter_text = str_squish(harry_potter_text)) %>% \n# Make a new column for chapter - if there is \"CHAPTER\" in the text column it will add \"CHAPTER\" to the chapter column and if not it will populate with NA:\n  mutate(chapter = case_when(\n    str_detect(harry_potter_text, pattern = \"CHAPTER\") ~ harry_potter_text,\n    TRUE ~ NA_character_\n  )) %>% \n# Populate the NA values in the chapter column with the chapter number until the next non-NA value:\n  fill(chapter) %>% \n# Separate the chapter column into the word chapter and the number: \n  separate(col = chapter, into = c(\"ch\", \"number\"), sep = \" \") %>% \n# Make numbers to lowercase numbers: \n  mutate(number = str_to_lower(number)) %>% \n# Remove un-necessary chapter column: \n  select(number, harry_potter_text) \n\n\n\nGet the Data into Tokenized Text Format:\nThen I use the unnest_tokens function to make each word in the book into a separate row, and then used anti_join() function to remove stop words:\n\n\nhide\n\n# Get this into tokenized text format:\nharry_potter_tokens <- harry_potter_tidy %>% \n  unnest_tokens(word, harry_potter_text) \n\n# Remove Stop Words:\nharry_nostop <- harry_potter_tokens %>% \n  anti_join(stop_words) \n\n\n\nData Wrangling for Word Frequency Word Cloud:\nI chose to make a word cloud of Chapter 15, “The Forbidden Forest”, using the 100 most used words in the chapter, so I needed to wrangle the data to get the frequencies of words in the chapter:\n\n\nhide\n\n#Count words by chapter:\nharry_counts <- harry_nostop %>% \n  count(number, word) \n\n#Find top 100 words in chapter 15:\nharry_ch15_top_100 <- harry_counts %>% \n# Only keep words from chapter 15:\n  filter(number == \"fifteen\") %>% \n# Arrange in decending order: \n  arrange(-n) %>% \n# Only keep top 100: \n  slice(1:100)\n\n\n\nMake a Word Cloud Using ggplot:\n\n\nhide\n\nggplot(data = harry_ch15_top_100, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n)) +\n  scale_size_area(max_size = 12) +\n  theme_minimal() +\n  scale_color_viridis(option = \"magma\",\n                      begin = .4,\n                      end = 1) +\n  theme(plot.background = element_rect(fill = \"gray18\",color = NA)) +\n  labs(title = \"100 Most Used Words in Harry Potter & The Sorcerer's Stone \\n Chapter 15: The Forbidden Forest\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, color = \"white\", face = \"bold\"))\n\n\n\n\nData Wrangling for Sentiment Analysis:\nI used the NRC lexicon for this sentiment analysis, which classifies words into ten emotional bins. First, I had to join the nrc lexicon with my tidy text data set using inner_join(). Then, I found counts of how many times the sentiment was reflected by words for each chapter. Since the chapter numbers were written out in words, I used the mutate() function to change the words to the number form\n\n\nhide\n\n#Inner join with NRC lexicon:\nharry_nrc <- harry_nostop %>% \n  inner_join(get_sentiments(\"nrc\")) \n\n# Count of times the 10 sentiments are reflected by words in each chapter:\nharry_nrc_counts <- harry_nrc %>% \n  mutate(number = case_when(number == \"one\" ~ \"1\",\n                            number == \"two\" ~ \"2\",\n                            number == \"three\" ~ \"3\",\n                            number == \"four\" ~ \"4\",\n                            number == \"five\" ~ \"5\",\n                            number == \"six\" ~ \"6\",\n                            number == \"seven\" ~ \"7\",\n                            number == \"eight\" ~ \"8\",\n                            number == \"nine\" ~ \"9\",\n                            number == \"ten\" ~ \"10\",\n                            number == \"eleven\" ~ \"11\",\n                            number == \"twelve\" ~ \"12\",\n                            number == \"thirteen\" ~ \"13\",\n                            number == \"fourteen\" ~ \"14\",\n                            number == \"fifteen\" ~ \"15\",\n                            number == \"sixteen\" ~ \"16\",\n                            number == \"seventeen\" ~ \"17\",\n                            )) %>% \n  count(sentiment, number) \n\n\n\nSentiment Analysis Data Visualization\nIn order to visualize the sentiment throughout the book, I plotted the sentiment counts per chapter using ggplot and I used facet_wrap() to separate the column graphs into individual graphs for each chapter.\n\n\nhide\n\n#Plot the counts of each of the 10 sentiments - see the emotion per chapter:\nggplot(data = harry_nrc_counts, aes(x = sentiment, y = n)) +\n  geom_col(aes(fill = number), show.legend = FALSE) +\n  theme_minimal() +\n  coord_flip() +\n  facet_wrap(~number, scales = \"free\") +\n  labs(title = \"Sentiment Analysis by Chapter of Harry Potter & \\n The Sorcerer's Stone\",\n       x = \" \", y = \" \") +\n  theme(plot.title = element_text(hjust = 0.5, size = 15, face = \"bold\")) +\n  theme(axis.text.x = element_text(size = 4, face = \"bold\")) +\n  theme(axis.text.y = element_text(size = 3.5, face = \"bold\")) +\n  theme(strip.text.x = element_text(size = 6, face = \"bold\"))\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-20-harry-potter-text-analysis/harry-potter-text-analysis_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-02-20T16:32:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-03-break-free-from-plastic/",
    "title": "Break Free From Plastic",
    "description": "1.26.2021 TidyTuesday - Choropleth Map Using `rnaturalearth`",
    "author": [
      {
        "name": "Kathleen Cotti",
        "url": {}
      }
    ],
    "date": "2021-02-03",
    "categories": [],
    "contents": "\nSummary:\nThis post explores how to analyze spatial data & build a choropleth map with the rnaturalearth package to participate in Tidy Tuesday 1.26.2021. This post shows how to load in tidyTuesday data, merge spatial data frames using left_join, and build a finalized choropleth map. I wanted to look at total plastic counts by country and display this on a spatial map. This is my first attempt at spacial data visualization and it was exciting to give spatial mapping a try for tidy tuesday!\nAt first, I had issues reading in a spatial data frame for the world map outline, but the rnaturalearth package has built in world maps. I made a world map showing the total plastic counts for 2020 and played around with the viridis package to update the color gradient of the map, and I learned how to change background color of a plot using plot.background in theme().\nReading in tidyTuesday data:\n\n\nhide\n\n# Read in the data:\nplastics <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-26/plastics.csv')\n\n\n\nData Wrangling:\nFor this map I wanted to look at total plastic counts for 2020, so I filtered the data using various functions from the dplyr package:\n\n\nhide\n\n# Data Wrangling - Sum of total plastic counts by country:\nplastics_summary <- plastics %>% \n# Remove NA values:\n  drop_na(grand_total) %>% \n# Filter to only keep counts from 2020:\n  filter(year == 2020) %>% \n# Group by country:\n  group_by(country) %>% \n# Find total plastics per country: \n  summarize(total_plastics = sum(grand_total)) \n\n\n\nMaking a chloropleth map using the ‘rnaturalearth’ package:\n\n\nhide\n\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\") \n\nworld_p <- plastics_summary %>% \n  left_join(world, ., by = c(\"name\" = \"country\"))\n\nggplot() +\n  geom_sf(data = world_p,\n          aes(fill = total_plastics), color = NA) +\n  scale_fill_viridis(option = \"viridis\",\n                     begin = 0.25,\n                     end = 1,\n                     breaks = c(10000, 20000,30000,40000,50000,60000),\n                     name = \"Total Plastics\")   +\n  theme_void() +\n  theme(plot.background = element_rect(fill = \"#f5f5f2\", color = NA)) +\n  labs(title = \"Worldwide Total Plastic Counts in 2020\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-03-break-free-from-plastic/break-free-from-plastic_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-02-20T15:58:06-08:00",
    "input_file": {}
  }
]
